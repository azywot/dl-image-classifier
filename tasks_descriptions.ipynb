{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"ovcQ4O8s7Ckm"},"source":["# Deep Learning - Project 2 - Convolutional Neural Networks\n","\n","## Basic information\n","\n","### Rules for completing the course\n","\n","- The grade for this project is $50\\%$ of the final grade for the Deep Learning course\n","- In order to pass the course, it is necessary to pass ($\\geq 50\\%$) each of the 2 projects\n","\n","### Purpose of the project\n","\n","- Getting acquainted with one of the most popular frameworks used in Deep Learning - TensorFlow\n","- Practical exercises related to Deep Learning (in particular Convolutional Neural Networks)\n","- Empirical confirmation of the influence of individual elements of the algorithm and data on the results (based on experimental comparative analyses)\n","- Improving programming skills\n","\n","### Project prerequisites\n","\n","- Student has the skills to write code in Python and is able to use Jupyter Notebook\n","- Student is able to search for information on the documentation of Python packages and use them in practice\n","- Student has knowledge of the basics of Deep Learning, in particular about the Convolutional Neural Networks (knowledge obtained during laboratories; introductory materials available on eKursy)\n","\n","### Implementation of the project\n","\n","- Individually or in groups of two\n","\n","### Parts of the project\n","\n","**Part 1** - Implementation of an image classifier ($40\\%$)\n","\n","**Part 2** - Experimental comparative analysis ($20\\%$ for each subtask):\n"," - Task's implementation ($10\\%$)\n"," - Task's description ($10\\%$)\n"," \n","Maximum rating for the project - $100\\%$.\n","\n","**Note:** Some tasks, in addition to implementation, will require you to describe your observations and thoughts. You can do it in the form of Markdowns (this is the preferred form) or prepare a separate file with the report (in that case - preferably PDF).\n"," \n","**Note:** To get points from a given subtask in Part 2, you must provide both an implementation and a description. If one of the task parts (implementation or description) is missing, the latter does not count. The percentages show how each part contributes to the grade.\n"," \n","**Note:** Without correctly completing the Part 1, it will likely be difficult (or even impossible) to complete tasks in the Part 2.\n","\n","### Technical requirements\n","\n","- The implementation should be done in Python, using TensorFlow, preferably with the Keras API. You can use TensorFlow with GPU support, but it's not mandatory. If you want to use a different package or technology, please consult it via e-mail or during the classes.\n","- Preferred form - Jupyter Notebook(s) (e.g. this one). It is also possible to implement in regular Python files (.py).\n","- Some tasks require displaying data, images, tables, plots, etc. Suggested support packages - NumPy, matplotlib, pandas, scikit-learn (you can use others if you want)\n","\n","### Deadline\n","\n","- **Saturday, 2023-01-28** (by the end of the day) \n","- In case of delay, each subsequent week started will lower the grade by $20\\%$. So the maximum grade for solutions sent in the period:\n","  - From 2023-01-29 to 2023-02-04 (by the end of the day) - $80\\%$\n","  - From 2023-02-05 to 2023-02-11 (by the end of the day) - $60\\%$\n","  - From 2023-02-12 - <font color='red'>$40\\%$ (project failed)</font>\n","  \n","\n","### Solution content\n","\n","**Please:**\n","\n","- Provide the source code performing tasks from the Part 1 and 2. As long as the Notebook is not too big (over a few MB) - please **don't** reset the Notebook(s) output. If I have problems while testing the code, I will be able to check your solutions at least by looking at the output. Alternatively, send me two notebooks - one with outputs filled in and the other without outputs.\n","- Attach also a list of packages with the versions you used when implementing the project (*pip freeze > requirements.txt*)\n","- If you are not using Markdowns in Notebook(s) or comments in case of regular Python files, please describe in the email where I should look for the code for specific parts of the tasks.\n","- If you decide to prepare a separate report (preffered PDF file), attach it as well.\n","- If the file structure is different than that proposed in the content of the tasks, provide information about the file structure - i.e. where should I put the dataset in order to run your code\n","- Provide any graphs, tables, files, resources etc. on the basis of which conclusions about the tasks were made, and are not visible in the Markdowns or output of the Notebook. Please describe in the report or in the e-mail what the attached materials relate to.\n","- Add files with model and parameters from Part 1, task 6) (as long as they do not occupy more than a few MB)\n","\n","**Please DON'T:**\n","\n","- Send the dataset - I'd rather avoid downloading mega- or gigabytes of data from each of you :)\n","\n","### Delivery method\n","\n","- Send the above-mentioned content (ZIP file preferred) to michal.wojcik@doctorate.put.poznan.pl\n","- Title: **[DL] Project 2 - [Student1_First_Name] [Student1_Surname] [Student1_ID], [Student2_First_Name] [Student2_Surname] [Student2_ID]**\n","- Title example: **[DL] Project 2 - Anna Nowak 123456, John Doe 789012**\n","\n","### Project modifications\n","\n","This document is a project proposal and describes the preferred way to complete and pass the second part of the Deep Learning Labs.\n","\n","If you believe that introducing minor (e.g. changing the dataset, new task definition) or major modifications to this project or changing the approach (e.g. implementing a different concept that solves another practical problem - mainly for those who are more experienced in Deep Learning) will be more interesting and will allow you to better understand and learn about previously unknown issues related to the basics of Deep Learning (in particular Convolutional Neural Networks), please do not hesitate to present your proposal and modify the project definition. \n","\n","In order for the proposed modifications to be accepted, the concept must be presented individually (by e-mail or during the class), then described in detail and approved via e-mail by the teacher."]},{"cell_type":"markdown","metadata":{"id":"Doj_TOx_7Ckq"},"source":["## Part 1 - Implementation of an image classifier (40%)\n","\n","The goal of the Part 1 is to implement an image classifier from scratch. The task consists in loading the data (images & labels), preprocessing, preparing the datasets for learning, preparing the model, conducting the learning process and evaluating the model on the test set. A more detailed scheme is presented below, broken down into subtasks. \n","\n","### Data\n","Proposed datasets:\n","\n","- Caltech-101 (131 MB)\n","- Caltech-256 (1.2 GB)\n","\n","**Link to datasets:** https://www.vision.caltech.edu/datasets/\n","\n","Both of the above collections consist of many JPG images containing an object from one of many classes. For each class of objects, there are from several dozen to several hundred examples of images.\n","\n","If you decide you want to use a different dataset, here are some requirements:\n","- The size of the images should not be smaller than 100x100 pixels\n","- Images should be in color (RGB/RGBA), not grayscale\n","- Images should be in such a format that they can be easily loaded into numpy.array of pixels (png, jpg)\n","- The collection should contain at least 20 classes with at least 80 examples for each class\n","- Images must be labeled - that is, assigned exactly to 1 of N decision classes\n","- The classification problem cannot be *trivial* (e.g. it is possible to assign photos to classes by calculating the average pixel color)\n","\n","As mentioned above, the proposed change of the dataset must be accepted by a teacher.\n","\n","### Before you start\n","\n","- Start by reading all the steps and see also the Part 2 - this will make it easier for you to plan the entire dataset processing flow and how to organize your code\n","- Implement tasks as a functions - it's always a good idea if your code is reusable and parameterizable, especially in that case, because some part of the source code from the Part 1 will be also useful in the Part 2\n","- Use Markdowns - they help organize the code and allow you to describe additional information. Each time the task includes a question or request to describe your conclusions, comments, observations, etc. your answer should be put in Markdowns (preferred form) or in the report file that you send after completing all of the tasks\n","- **Step verification** should also be implemented - it should be visible as the output of the cell(s) execution after implementing and executing the code that carries out the tasks specified in a given step\n","- The hints, order of subtasks, functions mentioned in the description below are only a suggestion - you can present your own approach and improvements, but in such a way that it is possible to observe the changes taking place between subsequent steps. Please, don't use *magic*, one-line functions that do all the steps at once - the aim of the project is also to familiarize you with the features offered by the packages (mainly TensorFlow with Keras API). If you have made any exceptions from the proposed approach (e.g. you loaded a dataset with the tensorflow_datasets package), please mention and describe this fact in the step 7)"]},{"cell_type":"markdown","metadata":{"id":"mAUptpKO7Ckr"},"source":["### Steps to follow:"]},{"cell_type":"markdown","metadata":{"id":"6XflhSsY7Ckr"},"source":["#### 1) Download the data and load it in the Notebook (5%)\n","\n","**Note:** The *standard* dataset loading sequence is described below. You can also use *tensorflow_datasets* package for this purpose.\n","\n","- Download the archive with the dataset\n","- Create directory \"data\" and extract downloaded files into\n","- Implement loading images and labels\n","- Each image should be represented in the form of numpy.array (shape: (height, width, channels))\n","- Load all interesting images and labels into two lists or numpy.arrays\n","\n","**Note:** Watch out for file extensions!\n","\n","**Step verification:**\n","\n","Display one of the loaded images, print out the shape of the image, check if the label is correct."]},{"cell_type":"markdown","metadata":{"id":"AjnIMWrs7Cks"},"source":["#### 2) Standardize the images (5%)\n","\n","**Note:** Some of these operations can be performed during step 1). If that's the case, don't do them again here.\n","\n","Unify the images:\n","\n","- Number and sequence of channels (RGB) if needed\n","- Images shape (e.g. $32 \\times 32 \\times 3$) - including channel convention (*channels_last* suggested)\n","- Standardization of pixel values, e.g. ($\\frac{x - \\mu}{\\sigma}$) - calculate $\\mu$ and $\\sigma$ for the whole dataset, separately for each channel\n","\n","**Step verification:**\n","\n","Check what the image you selected in step 1) looks like now."]},{"cell_type":"markdown","metadata":{"id":"NI724UHp7Cks"},"source":["#### 3) Divide the collection into Train and Test set (5%)\n","\n","\n","- Reduce the number of classes - filter the collection and leave images from ~15-25 classes, select those classes that have the largest number of examples. Make sure your collection is balanced (roughly the same number of samples for each class). You can do this by discarding classes that cause imbalance, or you can reduce the number of samples in larger classes.\n","- Randomly split the set into train (70%) and test (30%) set - X_train, X_test (images) and y_train, y_test (labels). (hint: check *sklearn.model_selection.train_test_split*)\n","- Make sure that the proportions of each class in both sets are more or less the same as in the whole set (hint: *stratify* parameter in *train_test_split*)\n","- Change labels in y vectors to one-hot encoding\n","- Ensure image and label collections are in the form of numpy.array\n","\n","**Note:** After selecting a subset of classes, check what images contain these classes - are they appropriate for the classification problem? \"BACKGROUND_Google\" is probably not the best choice :)\n","\n","**Step verification:**\n","\n","- Check the shape of the X_train, X_test (images) and the y_train, y_test (labels)\n","- Check on the example images if the label is in the correct form\n","- Check how many samples from each class are in particular subsets (train and test) and whether the proportions are the same in both subsets"]},{"cell_type":"markdown","metadata":{"id":"8rsVUOsQ7Ckt"},"source":["**4) Define the model (5%)**\n","\n","Suggestions (you should probably try different settings and choose the *best* one):\n","\n","- Activation functions - *ReLU*\n","- At least 3 *Convolutional blocks* - (Conv2D, Activation, BatchNormalization, Dropout, MaxPooling2D); Conv2D - *kernel=(3,3)*, *padding='same'*; MaxPooling2D - *pool_size=(2,2)*\n","- Flatten layer\n","- At least 2 layers Fully-Connected (Dense)\n","- Output layer - Dense with number_of_classes outputs (remember to use softmax)\n","\n","**Note:** You can add *Activation* as a separate layer or as *activation='relu'* parameter in Conv2D\n","\n","**Step verification:**\n","\n","Compile the model with *'adam'* optimizer, the Categorical Crossentropy as the loss function, and measure the accuracy value."]},{"cell_type":"markdown","metadata":{"id":"VfROJFnX7Ckt"},"source":["**5) Train the model (5%)**\n","\n","Suggested hyperparameters:\n","\n","- batch_size = 32\n","- epochs = 250 (or whatever you think is appropriate; first, just check if everything works on several epochs to save time)\n","- Monitor the value of measures for the test set\n","- Add [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) - stop training when there is no improvement in accuracy for the test set within 5 consecutive epochs (you can first test the behavior for patience = 1)\n","\n","**Step verification:**\n","\n","After completing the learning process, show:\n","\n","- Learning curves for loss_function and accuracy, changing over the epochs on train and test set\n","- Confusion matrix and calculate precision and recall for each class for test set\n","- Show few images from the test set, display the probabilities of assigning them to each class (at least two examples: one image example that was classified correctly and one which was classified incorrectly)\n","\n","**Note:** Functions to display learning curves and confusion matrices based on the model will be useful in the Part 2."]},{"cell_type":"markdown","metadata":{"id":"pO_rfCiB7Cku"},"source":["**6) Save the model to disk (5%)**\n","\n","- Prepare 2 functions - for saving the model and for loading the model\n","- Model structure should be saved as JSON file, model parameters in HDF5 file\n","\n","**Step verification:**\n","\n","Checking the operation of both functions by:\n","\n","- Save the model\n","- Load the model\n","- Make predictions on the loaded model for the test set\n","- Display the confusion matrix for the test set and compare it with the matrix obtained in the previous step to see if saving and loading worked properly"]},{"cell_type":"markdown","metadata":{"id":"GHi5Z3yU7Cku"},"source":["**7) Summary of the Part 1 - describe your observations (10%)**\n","\n","Describe your observations on the tasks performed. Supporting questions:\n","\n","- What kind of modifications have you made? Why? (*Describe, if you made any*)\n","- What results have you achieved?\n","- Is the underfitting or overfitting of the model visible? If so, what are your suggestions for solving this problem?\n","- Which class(es) the model had a trouble with? Which were the easiest for it? Which pair of classes were most often confused with each other? Can you guess why?\n","- What opportunities do you see for improvement?\n","\n","**Step verification:**\n","\n","Use Markdowns to describe your conclusions or put them into the report."]},{"cell_type":"markdown","metadata":{"id":"eMSoz6tJ7Ckv"},"source":["## Part 2 - Experimental comparative analysis\n","\n","The purpose of the Part 2 is to examine the dependence of the quality of the resulting model on factors such as hyperparameters, model structure, number of training data, number of decision classes, etc. Below is a suggestion of simple tasks - most of them consist of a simple experiment to compare several models that differ in some detail.\n","\n","### Before you start\n","\n","- You don't have to complete all of the tasks, you can choose the issues that seem interesting to you. You can also suggest your own ideas for the experiment definition (in this case - consult with the teacher). As mentioned above, each task is worth $20\\%$ of the project grade (the value of the last task is doubled, because it requires much more effort). Assuming you've completed the Part 1, as you may have already calculated:\n","  - You should complete at least 1 task *almost* correctly to pass ($40\\% + 20\\% = 60\\% \\geq 50\\%$)\n","  - You should complete at least 3 tasks correctly to get the maximum score ($40\\% + 3 \\cdot 20\\% = 100\\%$)\n","  \n","  Of course, you can complete more than 3 tasks, then the extra points will ensure that even if one of the tasks is not completely correct, you still have an opportunity to get $100\\%$.\n","- It is possible to combine tasks into one large experiment, because most of the tasks consist of comparing models that differ in one aspect. Thus, instead of conducting individual experiments, you can perform [grid search](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search) and modify a set of several parameters simultaneously. However, it is important to remember that the conclusions about a given experiment should take into account observations about each of the subtasks (different aspects).\n","- If you perform several experiments one after another, take into account the conclusions from the previous ones. For example, if in the first experiment you are testing the quality of different models, and in the second you want to check the effect of the dataset size on the results, then in the second task you should use the conclusions of the first experiment (i.e. choose the model that performed better). However, if applying the conclusions of the first study will significantly increase the time of the next experiment, you do not need to apply them to subsequent tasks.\n","- If you have not already done so, modify the functions in the Part 1 so that they can be used for the following subtasks. You can also rewrite the code and adapt it to the requirements of the tasks.\n","- Pay attention to the last task - it is more complicated and requires you to find additional information, therefore it is worth 50%.\n","\n","### Collecting experimental data\n","\n","In addition to implementing the code and running experiments to carry out a given task, you must draw conclusions about the results. In that case, it may be helpful to collect the following information about the compared models:\n","\n","- The number of network parameters (as a measure of the memory complexity of the model)\n","- Training time (as a measure of the time complexity of the model)\n","- Values of the obtained quality measures (loss function, accuracy - maybe other measures should be considered?) for train and  test sets in the resulting model\n","- Values of the loss function and measures in successive epochs (learning curves - can be useful in the context of discussing overfitting, optimization speed and model stability while learning)\n","- Confusion matrix (mainly in the test set, but maybe the train set matrix will help to better understand the learning process)\n","- Precision and recall (maybe some other measures?) for decision classes (assessment of classification difficulty for individual classes; comparison of the precision-recall curve between classes and between models)\n","- Display some examples that were correctly and incorrectly classified by one or more models (example-based explanation)\n","\n","In general, there are many possibilities to visualize data, to compare models with each other and draw conclusions from it. Only some suggestions are visible above, it is up to you what and how you decide to show in your conclusions and observations.\n","\n","### Task description\n","\n","Each of the following tasks assumes the implementation of an appropriate code that will perform a given experiment - and therefore it will probably modify the dataset or model (its structure or hyperparameters). After implementing the experiment, write:\n","- Which task did you choose?\n","- What are the differences between the models or data sets you tested?\n","\n","After obtaining the results for the compared models, datasets or any other aspect, describe your observations and conclusions about the experiment. Observations may in particular concern:\n","- The overall predictive ability of the model - which model is better, which model is worse, maybe one is better with class X and one with class Y, etc.\n","- The impact of the changes made on overfitting and underfitting\n","- Comparison of the duration of learning, the complexity of the model\n","- Assessment of whether it was profitable to make such a modification (or indicate the best option in your opinion)\n","- Learning curve shape - in which model the optimization was faster, whether the loss function and accuracy on the test set decreased steadily, were there any deviations, etc.\n","- Sample images where one model is better than the other - try to answer *why?*\n","- If something is not working well, write about your assumptions, why it failed and what could be changed. If you want, try the changes you suggest and also describe whether it has brought the desired effect.\n","\n","In general, it is worth describing everything that you find interesting in the context of the specific task. The description of the conclusions should be included in Markdown(s) (or in the report if you want to prepare one)."]},{"cell_type":"markdown","metadata":{"id":"M8Wd0IUK7Ckv"},"source":["### Tasks\n","\n","#### 1) The impact of the size of the training set on the results - choose one option:\n","- Compare the results achieved by the same model, e.g. for 15-20 classes that have ~ 80 samples and 15-20 classes that have ~ 30 samples\n","- You can also use the same classes and reduce the number of samples in the training set (e.g. 50, 40, 30, 20 samples in the training set)\n","- Another idea - modify the proportion of the division into Train and Test set (e.g. 80-20, 60-40, 40-60, 20-80)\n","\n","#### 2) The impact of the number of decision classes on the results (e.g. 10, 25, 50, all of the available classes)\n","\n","#### 3) Compare the models without and with Dropout with different rates (e.g. 0.1, 0.2, 0.5)\n","\n","#### 4) Compare the models without and with regularization (L1, L2, L1 + L2)\n","\n","#### 5) Compare the models without and with batch normalization (before or after the activation function)\n","\n","#### 6) Compare the models for different preprocessing approaches (perform operations separately per channel):\n","- Raw data - $X$\n","- Subtracting the mean ($X - \\mu$)\n","- Normalization ($\\frac{X - min}{max - min}$)\n","- Standardization ($\\frac{X - \\mu}{\\sigma}$)\n","\n","#### 7) Compare the models with different activation functions (ReLU, tanh, sigmoid - you can use others as well)\n","\n","#### 8) Compare the models with different Pooling layers (MaxPooling, AveragePooling; you can also check GlobalMaxPooling and GlobalAveragePooling after the last convolution layer)\n","\n","#### 9) Compare the models with different number of Convolutional blocks\n","\n","#### 10) Compare the models with different number of Neurons in each Convolutional block\n","\n","#### 11) Training with different batch sizes (e.g. 1, 16, 32, number_of_samples)\n","\n","#### 12) Training for unbalanced datasets  (Task double scored, worth 50%)\n","- Take one class with a lot of samples (e.g. in Caltech101 - airplanes with 800 samples) and 1 or several classes with significantly fewer samples (e.g. 1 class with ~80 samples or 5 classes with ~30 samples per class)\n","- Split the dataset proportionally in the Train and Test set\n","- Train the default model - what was your accuracy, precision, recall?\n","- Check what values would you get if you made a simple decision rule model which always classifies samples into the most numerous class? Is your model clearly better than it or has it achieved quite similar results?\n","- Find information about traning on the imbalanced dataset and how to deal with that problem e.g. change loss function, change quality measures, check how you can modify dataset\n","- Apply the changes and check if you managed to get a better model\n","- Describe your strategy for solving problems related to this issue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1IWAhcx7Ckw"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"data-mining2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2 | packaged by conda-forge | (main, Mar  8 2022, 15:52:24) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"e1791ee0b1058438a9e4dd50940b13379ea2066324df602589e5ef71bcea17c7"}}},"nbformat":4,"nbformat_minor":0}
